[{"uri":"https://www.didida.top/kubernetes/introduction/","title":"Introduction","tags":[],"description":"","content":"Kubernetes 服务发现和负载均衡 存储编排 自动部署和回滚 自动完成装箱计算 自我修复 密钥与配置管理 Kubernetes 组件 控制平面组件（Control Plane Components）\nkube-apiserver\netcd\nkube-scheduler\nkube-controller-manager\n从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。\n节点控制器（Node Controller）\n负责在节点出现故障时进行通知和响应。\n副本控制器（Replication Controller）\n负责为系统中的每个副本控制器对象维护正确数量的 Pod。\n端点控制器（Endpoints Controller）\n填充端点(Endpoints)对象(即加入 Service 与 Pod)。\n服务帐户和令牌控制器（Service Account \u0026amp; Token Controllers）\n为新的命名空间创建默认帐户和 API 访问令牌。\ncloud-controller-manager\n节点控制器（Node Controller）\n用于在节点终止响应后检查云提供商以确定节点是否已被删除。\n路由控制器（Route Controller）\n用于在底层云基础架构中设置路由。\n服务控制器（Service Controller）\n用于创建、更新和删除云提供商负载均衡器。\nNode 组件\n节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。\nkubelet\nkube-proxy\n容器运行时（Container Runtime）\nKubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。\n插件（Addons）\nDNS Web 界面（仪表盘） 容器资源监控 集群层面日志 节点（node） Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。\n"},{"uri":"https://www.didida.top/kubernetes/","title":"Kubernetes","tags":[],"description":"","content":"Kubernetes Install sudo cp * /usr/bin/ sudo mkdir -p /etc/systemd/system/kubelet.service.d echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward ebtables sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers sudo vi /etc/kubernetes/kubelet.env # add --fail-swap-on=false sudo kubeadm init \\ --kubernetes-version=v1.20.2 \\ --image-repository registry.aliyuncs.com/google_containers \\ --ignore-preflight-errors=all \\ --apiserver-advertise-address=10.10.10.11 \\ --pod-network-cidr=10.10.0.0/16 \\ --v=5 sudo kubeadm init \\ --kubernetes-version=v1.20.5 \\ --image-repository registry.aliyuncs.com/google_containers \\ --ignore-preflight-errors=all \\ --apiserver-advertise-address=192.168.0.10 \\ --pod-network-cidr=10.10.0.0/16 \\ --v=5 sudo systemctl enable --now kubelet # /run/flannel empty, just wait to finish kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # single node kubectl taint nodes --all node-role.kubernetes.io/master- ref: https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n[addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.103:6443 --token gd4fkv.f0rg29z4sjlrki6o \\ --discovery-token-ca-cert-hash sha256:a46f8c0ec42d43c2089b464d5af8050e135b934a63cd6759c12496f35185cee "},{"uri":"https://www.didida.top/docker/redis/","title":"Redis","tags":[],"description":"","content":"Redis docker pull redis docker run --name redis-bench \\ -v /home/Develop/data/redis/conf:/data \\ -p 127.0.0.1:6379:6379 \\ -d redis docker run --name redis-bench \\ -p 127.0.0.1:6379:6379 \\ -d redis Redis 集合(Set) 命令 描述 SADD key member1 [member2] 向集合添加一个或多个成员 SCARD key 获取集合的成员数 SDIFF key1 [key2] 返回第一个集合与其他集合之间的差异 SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 SINTER key1 [key2] 返回给定所有集合的交集 SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 SISMEMBER key member 判断 member 元素是否是集合 key 的成员 SMEMBERS key 返回集合中的所有成员 SMOVE source destination member 将 member 元素从 source 集合移动到 destination 集合 SPOP key 移除并返回集合中的一个随机元素 SRANDMEMBER key [count] 返回集合中一个或多个随机数 SREM key member1 [member2] 移除集合中一个或多个成员 SUNION key1 [key2] 返回所有给定集合的并集 SUNIONSTORE destination key1 [key2] 所有给定集合的并集存储在 destination 集合中 SSCAN key cursor [MATCH pattern] [COUNT count] 迭代集合中的元素 ref: https://www.runoob.com/redis/redis-sets.html\nRedis 有序集合(sorted set) 序号 命令 描述 1 ZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员，或者更新已存在成员的分数 2 ZCARD key 获取有序集合的成员数 3 ZCOUNT key min max 计算在有序集合中指定区间分数的成员数 4 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量 increment 5 ZINTERSTORE destination numkeys key [key \u0026hellip;] 计算给定的一个或多个有序集的交集并将结果集存储在新的有序集合 destination 中 6 ZLEXCOUNT key min max 在有序集合中计算指定字典区间内成员数量 7 ZRANGE key start stop [WITHSCORES] 通过索引区间返回有序集合指定区间内的成员 8 ZRANGEBYLEX key min max [LIMIT offset count] 通过字典区间返回有序集合的成员 9 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 通过分数返回有序集合指定区间内的成员 10 ZRANK key member 返回有序集合中指定成员的索引 11 ZREM key member [member \u0026hellip;] 移除有序集合中的一个或多个成员 12 ZREMRANGEBYLEX key min max 移除有序集合中给定的字典区间的所有成员 13 ZREMRANGEBYRANK key start stop 移除有序集合中给定的排名区间的所有成员 14 ZREMRANGEBYSCORE key min max 移除有序集合中给定的分数区间的所有成员 15 ZREVRANGE key start stop [WITHSCORES] 返回有序集中指定区间内的成员，通过索引，分数从高到低 16 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回有序集中指定分数区间内的成员，分数从高到低排序 17 ZREVRANK key member 返回有序集合中指定成员的排名，有序集成员按分数值递减(从大到小)排序 18 ZSCORE key member 返回有序集中，成员的分数值 19 ZUNIONSTORE destination numkeys key [key \u0026hellip;] 计算给定的一个或多个有序集的并集，并存储在新的 key 中 20 ZSCAN key cursor [MATCH pattern] [COUNT count] 迭代有序集合中的元素（包括元素成员和元素分值） ref: https://www.runoob.com/redis/redis-sorted-sets.html\n"},{"uri":"https://www.didida.top/docker/rabbitmq/","title":"Rabbitmq","tags":[],"description":"","content":"RabbitMQ docker pull rabbitmq:management docker run --name rabbitmq-bench \\ -p 127.0.0.1:15672:15672 \\ -p 127.0.0.1:5672:5672 \\ -p 127.0.0.1:15692:15692 \\ -e RABBITMQ_DEFAULT_USER=admin \\ -e RABBITMQ_DEFAULT_PASS=123456 \\ -d rabbitmq:management exchange ref: https://www.rabbitmq.com/tutorials/tutorial-three-go.html\ntypes:\ndirect\ntopic\nheaders\nfanout\nIt just broadcasts all the messages it receives to all the queues it knows\nqueue producer consumer "},{"uri":"https://www.didida.top/operate-system/introduction/","title":"Introduction","tags":[],"description":"","content":"操作系统的主要功能 从资源管理器的观点来看，操作系统的任务是高效地管理整个系统的各个部分； 从扩展机的观点来看，其任务是为用户提供一台比物理计算机更易于使用的虚拟计算机； 系统调用 与进程有关，与文件系统有关\n与进程的创建和终止有关 处理信号 针对文件读写 进行目录管理 对信息进行保护 用于时间管理 操作系统结构 整体结构\n整个操作系统是一组函数的集合，其中每个函数在需要的时候可以去调用任何其他的函数\n分层结构\n把整个操作系统组织成一个层次结构，每一层软件都是在它的下层软件的基础上构造起来的\n虚拟机\n外核\n外核程序管理资源分配\n客户-服务器模型\n消息请求回复\n操作系统 进程管理 I/O设备管理 存储管理 文件管理 进程 通信\n调度\n时钟中断 非抢占式调度算法 抢占式调度算法\n所有系统\n公平 策略强制执行 平衡\n批处理系统\n吞吐量 周转时间 CPU利用率\n非抢占式调度算法 先到先服务 最短作业优先 抢占式调度算法 最短剩余时间优先 三级调度 准入调度器 内存调度器 CPU调度器 交互式系统\n响应时间 均衡性\n两级调度： 内存调度 CPU调度\n时间片轮转调度 优先级调度 多重队列 最短进程优先 保证调度算法 彩票调度算法 公平分享调度 实时系统\n满足截至时间 可预测性\n系统： 硬实时 软实时\n时间： 周期性 非周期性\n策略与机制\n线程调度\n"},{"uri":"https://www.didida.top/operate-system/","title":"Operate System","tags":[],"description":"","content":""},{"uri":"https://www.didida.top/docker/mysql/","title":"Mysql","tags":[],"description":"","content":"mysql docker pull mysql:5.7 # 拉取 mysql 5.7 docker pull mysql # 拉取最新版mysql镜像 docker run -p 3306:3306 --name mysql-bench \\ -v /run/media/mike/Data/Develop/data/mysql/tmp:/tmp \\ -e MYSQL_ROOT_PASSWORD=123456 \\ -d mysql docker run -p 127.0.0.1:3306:3306 --name mysql-bench \\ -v /home/mike/Develop/data/mysql/conf:/etc/mysql \\ -v /home/mike/Develop/data/mysql/mysql:/var/lib/mysql \\ -v /home/mike/Develop/data/mysql/mysql-files:/var/lib/mysql-files \\ -v /home/mike/Develop/data/mysql/logs:/var/log/mysql \\ -e MYSQL_ROOT_PASSWORD=123456 \\ -d mysql mysql -h 127.0.0.1 -P 3306 -u root -p 123456 ref: https://www.cnblogs.com/sablier/p/11605606.html\nbench sysbench sysbench /usr/share/sysbench/oltp_read_write.lua \\ --tables=5 \\ --table_size=100 \\ --mysql-user=root \\ --mysql-password=123456 \\ --mysql-host=127.0.0.1 \\ --mysql-port=3306 \\ --mysql-db=sysbench_test \\ prepare sysbench /usr/share/sysbench/oltp_read_write.lua \\ --tables=1 \\ --table_size=10000000 \\ --mysql-user=root \\ --mysql-password=123456 \\ --mysql-host=127.0.0.1 \\ --mysql-port=3306 \\ --mysql-db=sysbench_test \\ prepare sysbench /usr/share/sysbench/oltp_read_write.lua \\ --mysql-user=root \\ --mysql-password=123456 \\ --mysql-host=127.0.0.1 \\ --mysql-port=3306 \\ --mysql-db=sysbench_test \\ --tables=5 \\ --table_size=100 \\ --threads=10 \\ --time=30 \\ --report-interval=3 \\ run ref: https://www.cnblogs.com/jinjiangongzuoshi/p/13883129.html\n"},{"uri":"https://www.didida.top/docker/monitor/","title":"Monitor","tags":[],"description":"","content":"Monitor docker pull prom/prometheus docker pull prom/node-exporter docker pull grafana/grafana docker run --name archlinux \\ -p 127.0.0.1:9100:9100 \\ -v /proc:/host/proc:ro \\ -v /sys:/host/sys:ro \\ -v /:/rootfs:ro \\ --net=host \\ -d prom/node-exporter docker run --name mysqld-exporter-mysql-bench \\ -p 127.0.0.1:9104:9104 \\ --link=mysql-bench:mysql \\ -e DATA_SOURCE_NAME=\u0026#34;root:123456@(mysql:3306)/\u0026#34; \\ -d prom/mysqld-exporter docker run --name prometheus \\ -p 127.0.0.1:9090:9090 \\ -v /home/mike/Develop/data/prometheus/conf/prometheus.yml:/etc/prometheus/prometheus.yml \\ -d prom/prometheus docker run --name=grafana \\ -p 127.0.0.1:3000:3000 \\ -v /home/mike/Develop/data/grafana/data:/var/lib/grafana \\ -d grafana/grafana # admin:admin -\u0026gt; admin:123456 # dashboard # 系统监控 1 Node Exporter for Prometheus Dashboard CN v20201010 https://grafana.com/grafana/dashboards/8919 # RabbitMQ-Overview https://grafana.com/grafana/dashboards/10991 "},{"uri":"https://www.didida.top/docker/gitlab/","title":"Gitlab","tags":[],"description":"","content":"Gitlab docker run --name gitlab \\ -p 127.0.0.1:22:22 \\ -p 127.0.0.1:443:443 \\ -p 127.0.0.1:80:80 \\ -d gitlab/gitlab-ce docker run --name gitlab \\ -p 127.0.0.1:22:22 \\ -p 127.0.0.1:443:443 \\ -p 127.0.0.1:80:80 \\ --hostname gitlab.org \\ -v /home/Mount/Develop/data/gitlab/etc:/etc/gitlab \\ -v /home/Mount/Develop/data/gitlab/opt:/var/opt/gitlab \\ -v /home/Mount/Develop/data/gitlab/log:/var/log/gitlab \\ -d gitlab/gitlab-ce Kubernetes cluster https://hub.docker.com/r/gitlab/gitlab-ce\n"},{"uri":"https://www.didida.top/docker/elk/","title":"Elk","tags":[],"description":"","content":"ELK Run docker pull sebp/elk docker run --name elk \\ -p 127.0.0.1:5601:5601 \\ -p 127.0.0.1:9200:9200 \\ -p 127.0.0.1:5044:5044 \\ -e ES_HEAP_SIZE=\u0026#34;2g\u0026#34; \\ -e LS_HEAP_SIZE=\u0026#34;1g\u0026#34; \\ -v /home/Develop/data/elk/es-data:/var/lib/elasticsearch \\ -v /home/Develop/data/elk/logstash:/etc/logstash \\ --link redis-bench:redis \\ -d sebp/elk # -e TZ=\u0026#34;Asia/Shanghai\u0026#34; \\ ref: https://elk-docker.readthedocs.io/#running-with-docker-compose\nissue max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\nsudo vi /etc/sysctl.conf vm.max_map_count=655360 sudo sysctl -p ref: https://blog.csdn.net/tl1242616458/article/details/105602361/\n"},{"uri":"https://www.didida.top/docker/etcd/","title":"Etcd","tags":[],"description":"","content":"Etcd Install docker pull quay.io/coreos/etcd docker run --name etcd-node1 \\ -p 127.0.0.1:2379:2379 \\ -p 127.0.0.1:2380:2380 \\ -e ALLOW_NONE_AUTHENTICATION=yes \\ -e ETCD_NAME=node1 \\ -d bitnami/etcd docker run --name e3w \\ -p 127.0.0.1:8030:8080 \\ --link etcd-node1:etcd \\ -d soyking/e3w docker run --name etcd-node1 \\ -p 127.0.0.1:2379:2379 \\ -p 127.0.0.1:2380:2380 \\ -e ETCD_LISTEN_CLIENT_URLS=http://127.0.0.1:2379 \\ -e ETCD_LISTEN_PEER_URLS=http://127.0.0.1:2380 \\ -e ETCD_INITIAL_ADVERTISE_PEER_URLS=http://127.0.0.1:2380 \\ -e ALLOW_NONE_AUTHENTICATION=yes \\ -e ETCD_INITIAL_CLUSTER=node1=http://127.0.0.1:2380 \\ -e ETCD_NAME=node1 \\ -d bitnami/etcd docker run --name etcd-node1 \\ -p 127.0.0.1:2379:2379 \\ -p 127.0.0.1:2380:2380 \\ -d quay.io/coreos/etcd # In this example, all three of these containers will be running on the same machine. Here are the three docker run commands that will get the cluster up and running. First, export a variable with your public IP address: export PUBLIC_IP=54.196.167.255 # And then start up our leader + two followers: docker run -d -p 8001:8001 -p 5001:5001 quay.io/coreos/etcd:v0.4.6 -peer-addr ${PUBLIC_IP}:8001 -addr ${PUBLIC_IP}:5001 -name etcd-node1 docker run -d -p 8002:8002 -p 5002:5002 quay.io/coreos/etcd:v0.4.6 -peer-addr ${PUBLIC_IP}:8002 -addr ${PUBLIC_IP}:5002 -name etcd-node2 -peers ${PUBLIC_IP}:8001,${PUBLIC_IP}:8002,${PUBLIC_IP}:8003 docker run -d -p 8003:8003 -p 5003:5003 quay.io/coreos/etcd:v0.4.6 -peer-addr ${PUBLIC_IP}:8003 -addr ${PUBLIC_IP}:5003 -name etcd-node3 -peers ${PUBLIC_IP}:8001,${PUBLIC_IP}:8002,${PUBLIC_IP}:8003 ref: https://www.cnblogs.com/hatlonely/p/11945491.html\nref: https://coreos.com/blog/running-etcd-in-containers.html\n"},{"uri":"https://www.didida.top/docker/","title":"Docker","tags":[],"description":"","content":"Docker Config sudo vim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;graph\u0026#34;: \u0026#34;/home/Develop/data/docker\u0026#34; } sudo systemctl restart docker sudo usermod -a -G users $USER "},{"uri":"https://www.didida.top/compiler/introduction/","title":"简介","tags":[],"description":"","content":"编程语言是向人和计算机描述计算过程的记号，程序必须翻译成可以被计算机执行的形式，完成这个翻译过程的软件系统叫做编译器。\n语言处理 编译器的结构 分析（analysis） 合成（synthesis） 编程语言的演变 制作编译器的科学 编译器技术的应用 编程语言基础 总结 "},{"uri":"https://www.didida.top/compiler/","title":"编译器","tags":[],"description":"","content":""},{"uri":"https://www.didida.top/compiler/a-simple-syntax-directed-translator/","title":"简单的语法制导翻译","tags":[],"description":"","content":"语法制导翻译 介绍 语法定义 语法制导翻译 语法解析 简单表达式的翻译 词法分析 符号表 中间代码生成 "},{"uri":"https://www.didida.top/golang/quick-start/install/","title":"安装","tags":[],"description":"","content":"ref: https://golang.org/\n"},{"uri":"https://www.didida.top/golang/","title":"Golang","tags":[],"description":"","content":"Golang Go 语言学习中的整理和总结。\nQuick Start\ngo 语法和语言特性的整理\n"},{"uri":"https://www.didida.top/leetcode/","title":"LeetCode","tags":[],"description":"","content":"LeetCode LeetCode 刷题记录\n"},{"uri":"https://www.didida.top/","title":"主页","tags":[],"description":"","content":"简介 这是一个记录程序员个人成长的网站，也是督促自己学习进步的一个项目，旨在记录学习过程中的心得体会，避免走第二次弯路。\n"},{"uri":"https://www.didida.top/golang/quick-start/","title":"入门","tags":[],"description":"","content":"Golang test\n"},{"uri":"https://www.didida.top/golang/quick-start/quick-start/","title":"语言特性","tags":[],"description":"","content":"Go 语言学习 "},{"uri":"https://www.didida.top/kubernetes/deployment/gitlab/gitlab/","title":"","tags":[],"description":"","content":"Gitlab ArchLinux Install sudo pacman -S postgresql-libs yay -S gitlab Configure your /etc/webapps/gitlab/gitlab.yml Set up your redis to run on /run/redis/redis.sock or configure gitlab to use redis TCP Put a secret bytestring to /etc/webapps/gitlab/secret Copy /usr/share/webapps/gitlab/config/secrets.yml.example to /etc/webapps/gitlab/secrets.yml and configure it Setup the database: $ (cd /usr/share/webapps/gitlab \u0026amp;\u0026amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:setup) Finally run the following commands to check your installation: $ (cd /usr/share/webapps/gitlab \u0026amp;\u0026amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:env:info) $ (cd /usr/share/webapps/gitlab \u0026amp;\u0026amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:check) Optional dependencies for gitlab postgresql: database backend python-docutils: reStructuredText markup language support [installed] smtp-server: mail server in order to receive mail notifications sudo cp /usr/share/webapps/gitlab/config/secrets.yml.example /etc/webapps/gitlab/secrets.yml # postgresql sudo systemctl start postgresql.service sudo su - postgres -c \u0026#34;initdb --locale zh_CN.UTF-8 -D \u0026#39;/var/lib/postgres/data\u0026#39;\u0026#34; sudo usermod -a -G postgres $USER psql -U postgres psql -d template1 template1=# CREATE USER your_username_here WITH PASSWORD \u0026#39;your_password_here\u0026#39;; template1=# ALTER USER your_username_here SUPERUSER; template1=# CREATE DATABASE gitlabhq_production OWNER your_username_here; template1=# \\q # redis sudo vi /etc/redis.conf unixsocket /var/run/redis/redis.sock unixsocketperm 770 sudo systemctl restart redis vi /etc/webapps/gitlab/gitlab.yml host: gitlab.org hexdump -v -n 64 -e \u0026#39;1/1 \u0026#34;%02x\u0026#34;\u0026#39; /dev/urandom \u0026gt; /etc/webapps/gitlab/secret chmod 640 /etc/webapps/gitlab/secret hexdump -v -n 64 -e \u0026#39;1/1 \u0026#34;%02x\u0026#34;\u0026#39; /dev/urandom \u0026gt; /etc/webapps/gitlab-shell/secret chmod 640 /etc/webapps/gitlab-shell/secret vi /etc/webapps/gitlab/secrets.yml production: secret_key_base: /etc/webapps/gitlab/secret db_key_base: /etc/webapps/gitlab-shell/secret sudo vi /etc/webapps/gitlab/resque.yml development: url: unix:/var/run/redis/redis.sock test: url: unix:/var/run/redis/redis.sock production: url: unix:/var/run/redis/redis.sock sudo chown -R gitlab /usr/share/webapps/gitlab/tmp sudo chmod -R u+rwX /usr/share/webapps/gitlab/tmp sudo systemctl start gitlab-gitaly.service cd /usr/share/webapps/gitlab # sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:setup sudo -u gitlab DISABLE_DATABASE_ENVIRONMENT_CHECK=1 $(cat environment | xargs) bundle exec rake gitlab:setup sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:env:info sudo systemctl start gitlab-workhorse.service sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:check # Init script exists? ... no :) this is ok for we using systemd # sudo certbot certonly -d *.didida.top sudo certbot certonly -d didida.top openssl genpkey -algorithm RSA -out server.key -pkeyopt rsa_keygen_bits:2048 openssl req -new -key ca.key -out ca.csr openssl x509 -req -days 3650 -in ca.csr -signkey ca.key -out ca.crt sudo systemctl start nginx redis postgresql gitlab-gitaly gitlab-workhorse gitlab.target sudo systemctl status nginx redis postgresql gitlab-gitaly gitlab-workhorse sudo systemctl stop nginx redis postgresql gitlab-gitaly gitlab-workhorse ref: https://wiki.archlinux.org/index.php?title=GitLab\u0026amp;oldid=647720#Configuration\nDocker Install docker run --name gitlab \\ -p 127.0.0.1:22:22 \\ -p 127.0.0.1:443:443 \\ -p 127.0.0.1:80:80 \\ -d gitlab/gitlab-ce export DATA=\u0026lt;ABSOLUTE PATH\u0026gt; docker run --name gitlab \\ -p 127.0.0.1:22:22 \\ -p 127.0.0.1:443:443 \\ -p 127.0.0.1:80:80 \\ --hostname gitlab.org \\ -v $DATA/gitlab/etc:/etc/gitlab \\ -v $DATA/gitlab/opt:/var/opt/gitlab \\ -v $DATA/gitlab/log:/var/log/gitlab \\ -d gitlab/gitlab-ce # browser http://localhost:80 Add Kubernetes Cluster login -\u0026gt; admin root -\u0026gt; Admin Area -\u0026gt; Kubernetes -\u0026gt; Integrade with a cluster certificate -\u0026gt; Connect existiing cluseter\n# https://gitlab.org/admin/application_settings/network#js-outbound-settings # check Allow requests to the local network from web hooks and services # get kubernetes api url, should be exactly the same kubectl cluster-info | grep \u0026#39;Kubernetes master\u0026#39; | awk \u0026#39;/http/ {print $NF}\u0026#39; # get kubernetes secrets kubectl get secrets kubectl get secret \u0026lt;SECRETS NAME\u0026gt; -o jsonpath=\u0026#34;{[\u0026#39;data\u0026#39;][\u0026#39;ca\\.crt\u0026#39;]}\u0026#34; | base64 --decode vi gitlab-admin-service-account.yaml apiVersion: v1 kind: ServiceAccount metadata: name: gitlab namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: gitlab-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: gitlab namespace: kube-system kubectl apply -f gitlab-admin-service-account.yaml # get kubernetes token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep gitlab | awk \u0026#39;{print $1}\u0026#39;) # add gitlab prometheus PersistentVolume, no need to claim, just add kubectl apply -f gitlab-pv-volume.yaml ref: https://docs.gitlab.com/ee/user/project/clusters/add_remove_clusters.html\nref: https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-persistent-volume-storage/\n"},{"uri":"https://www.didida.top/kubernetes/deployment/istio/istio/","title":"","tags":[],"description":"","content":"# https://istio.io/latest/docs/examples/bookinfo/ istioctl install --set profile=demo -d manifests -y kubectl label namespace default istio-injection=enabled kubectl apply -f bookinfo.yaml kubectl get services kubectl get pods kubectl exec \u0026#34;$(kubectl get pod -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -c ratings -- curl -sS productpage:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; kubectl apply -f bookinfo-gateway.yaml kubectl get gateway kubectl get svc istio-ingressgateway -n istio-system export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39;) echo $INGRESS_PORT export INGRESS_HOST=192.168.0.10 export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT curl -s \u0026#34;http://${GATEWAY_URL}/productpage\u0026#34; | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; "},{"uri":"https://www.didida.top/kubernetes/deployment/istio/manifests/charts/install-openshift/","title":"","tags":[],"description":"","content":"Installing Istio on OpenShift using Helm Note: Be aware of the platform setup required for OpenShift when installing Istio.\nTo install with Helm, you must first create the namespace that you wish to install in if the namespace does not exist already. The default namespace used is istio-system and can be created as follows:\nkubectl create namespace istio-system The installation process using the Helm charts is as follows:\nbase chart creates cluster-wide CRDs, cluster bindings and cluster resources. It is possible to change the namespace from istio-system but it is not recommended. helm install istio-base -n istio-system manifests/charts/base istio-cni chart installs the CNI plugin. This should be installed after the base chart and prior to istiod chart. Need to add --set istio_cni.enabled=true to the istiod install to enable its usage. helm install istio-cni -n kube-system manifests/charts/istio-cni -f manifests/charts/global.yaml --set cni.cniBinDir=\u0026#34;/var/lib/cni/bin\u0026#34; --set cni.cniConfDir=\u0026#34;/etc/cni/multus/net.d\u0026#34; --set cni.chained=false --set cni.cniConfFileName=\u0026#34;istio-cni.conf\u0026#34; --set cni.excludeNamespaces[0]=\u0026#34;istio-system\u0026#34; --set cni.excludeNamespaces[1]=\u0026#34;kube-system\u0026#34; --set cni.repair.enabled=false --set cni.logLevel=info istio-control/istio-discovery chart installs a revision of istiod. helm install -n istio-system istio-17 manifests/charts/istio-control/istio-discovery --set istio_cni.enabled=true --set global.jwtPolicy=first-party-jwt --set sidecarInjectorWebhook.injectedAnnotations.\u0026#34;k8s\\.v1\\.cni\\.cncf\\.io/networks\u0026#34;=\u0026#34;istio-cni\u0026#34; gateways charts install a load balancer with ingress and egress. Ingress secrets and access should be separated from the control plane.\nhelm install -n istio-system istio-ingress manifests/charts/gateways/istio-ingress -f manifests/charts/global.yaml --set global.jwtPolicy=first-party-jwt Egress secrets and access should be separated from the control plane.\nhelm install -n istio-system istio-egress manifests/charts/gateways/istio-egress -f manifests/charts/global.yaml --set global.jwtPolicy=first-party-jwt "},{"uri":"https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme-helm3/","title":"","tags":[],"description":"","content":"Helm v3 support Install The Helm charts are supported both by Helm v2 and Helm v3. Please do not introduce Helm v3 specific changes as many users are still using Helm v2 and the operator is currently using the Helm v2 code to generate.\nTo install with Helm v3, you must first create the namespace that you wish to install in if the namespace does not exist already. The default namespace used is istio-system and can be created as follows:\nkubectl create namespace istio-system The charts are as follows:\nbase creates cluster-wide CRDs, cluster bindings and cluster resources. It is possible to change the namespace from istio-system but it is not recommended. helm install istio-base -n istio-system manifests/charts/base istio-control/istio-discovery installs a revision of istiod. You can install it multiple times, with different revisions. helm install -n istio-system istio-17 manifests/charts/istio-control/istio-discovery helm install -n istio-system istio-canary manifests/charts/istio-control/istio-discovery \\ --set revision=canary helm install -n istio-system istio-mytest manifests/charts/istio-control/istio-discovery \\ --set revision=mytest gateways install a load balancer with ingress and egress. You can install it multiple times with different revisions but they must be installed in separate namespaces. Ingress secrets and access should be separated from the control plane.\nhelm install -n istio-system istio-ingress manifests/charts/gateways/istio-ingress kubectl create ns istio-ingress-canary helm install -n istio-ingress-canary istio-ingress-canary manifests/charts/gateways/istio-ingress \\ --set revision=canary Egress secrets and access should be separated from the control plane.\nhelm install -n istio-system istio-egress manifests/charts/gateways/istio-egress kubectl create ns istio-egress-canary helm install -n istio-egress-canary istio-egress-canary manifests/charts/gateways/istio-egress \\ --set revision=canary \u0026lsquo;istio-cni\u0026rsquo; installs the CNI plugin. This should be installed after the \u0026lsquo;base\u0026rsquo; chart and prior to istiod. Need to add --set istio_cni.enabled=true to the istiod install to enable its usage. helm install istio-cni -n istio-system manifests/charts/istio-cni Namespaces One of the changes in Helm v3 is that the namespace is no longer created on the fly when installing a chart. This means that the namespace being used needs to be created prior to installing the charts if it does not exist already. If the default istio-system namespace if not being used then you need to add the setting --set global.istioNamespace=\u0026lt;namespace\u0026gt; to the installs, to match the control plane namespace.\n"},{"uri":"https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme/","title":"","tags":[],"description":"","content":"Istio Installer Note: If making any changes to the charts or values.yaml in this dir, first read UPDATING-CHARTS.md\nIstio installer is a modular, \u0026lsquo;a-la-carte\u0026rsquo; installer for Istio. It is based on a fork of the Istio helm templates, refactored to increase modularity and isolation.\nGoals:\nImprove upgrade experience: users should be able to gradually roll upgrades, with proper canary deployments for Istio components. It should be possible to deploy a new version while keeping the stable version in place and gradually migrate apps to the new version.\nMore flexibility: the new installer allows multiple \u0026rsquo;environments\u0026rsquo;, allowing applications to select a set of control plane settings and components. While the entire mesh respects the same APIs and config, apps may target different \u0026rsquo;environments\u0026rsquo; which contain different instances and variants of Istio.\nBetter security: separate Istio components reside in different namespaces, allowing different teams or roles to manage different parts of Istio. For example, a security team would maintain the root CA and policy, a telemetry team may only have access to Prometheus, and a different team may maintain the control plane components (which are highly security sensitive).\nThe install is organized in \u0026rsquo;environments\u0026rsquo; - each environment consists of a set of components in different namespaces that are configured to work together. Regardless of \u0026rsquo;environment\u0026rsquo;, workloads can talk with each other and obey the Istio configuration resources, but each environment can use different Istio versions and different configuration defaults.\nistioctl kube-inject or the automatic sidecar injector are used to select the environment. In the case of the sidecar injector, the namespace label istio-env: \u0026lt;NAME_OF_ENV\u0026gt; is used instead of the conventional istio-injected: true. The name of the environment is defined as the namespace where the corresponding control plane components (config, discovery, auto-injection) are running. In the examples below, by default this is the istio-control namespace. Pod annotations can also be used to select a different \u0026rsquo;environment\u0026rsquo;.\nInstalling The new installer is intended to be modular and very explicit about what is installed. It has far more steps than the Istio installer - but each step is smaller and focused on a specific feature, and can be performed by different people/teams at different times.\nIt is strongly recommended that different namespaces are used, with different service accounts. In particular access to the security-critical production components (root CA, policy, control) should be locked down and restricted. The new installer allows multiple instances of policy/control/telemetry - so testing/staging of new settings and versions can be performed by a different role than the prod version.\nThe intended users of this repo are users running Istio in production who want to select, tune and understand each binary that gets deployed, and select which combination to use.\nNote: each component can be installed in parallel with an existing Istio 1.0 or 1.1 install in istio-system. The new components will not interfere with existing apps, but can interoperate and it is possible to gradually move apps from Istio 1.0/1.1 to the new environments and across environments ( for example canary -\u0026gt; prod )\nNote: there are still some cluster roles that may need to be fixed, most likely cluster permissions will need to move to the security component.\nEverything is Optional Each component in the new installer is optional. Users can install the component defined in the new installer, use the equivalent component in istio-system, configured with the official installer, or use a different version or implementation.\nFor example you may use your own Prometheus and Grafana installs, or you may use a specialized/custom certificate provisioning tool, or use components that are centrally managed and running in a different cluster.\nThis is a work in progress - building on top of the multi-cluster installer.\nAs an extreme, the goal is to be possible to run Istio workloads in a cluster without installing any Istio component in that cluster. Currently the minimum we require is the security provider (node agent or citadel).\nInstall Istio CRDs This is the first step of the install. Please do not remove or edit any CRD - config currently requires all CRDs to be present. On each upgrade it is recommended to reapply the file, to make sure you get all CRDs. CRDs are separated by release and by component type in the CRD directory.\nIstio has strong integration with certmanager. Some operators may want to keep their current certmanager CRDs in place and not have Istio modify them. In this case, it is necessary to apply CRD files individually.\nkubectl apply -k github.com/istio/installer/base or\nkubectl apply -f base/files Install Istio-CNI This is an optional step - CNI must run in a dedicated namespace, it is a \u0026lsquo;singleton\u0026rsquo; and extremely security sensitive. Access to the CNI namespace must be highly restricted.\nNOTE: The environment variable ISTIO_CLUSTER_ISGKE is assumed to be set to true if the cluster is a GKE cluster.\nISTIO_CNI_ARGS= # TODO: What k8s data can we use for this check for whether GKE? if [[ \u0026#34;${ISTIO_CLUSTER_ISGKE}\u0026#34; == \u0026#34;true\u0026#34; ]]; then ISTIO_CNI_ARGS=\u0026#34;--set cni.cniBinDir=/home/kubernetes/bin\u0026#34; fi iop kube-system istio-cni $IBASE/istio-cni/ ${ISTIO_CNI_ARGS} TODO. It is possible to add Istio-CNI later, and gradually migrate.\nInstall Control plane This can run in any cluster. A mesh should have at least one cluster should run Pilot or equivalent XDS server, and it is recommended to have Pilot running in each region and in multiple availability zones for multi cluster.\niop istio-control istio-discovery $IBASE/istio-control/istio-discovery \\ --set global.istioNamespace=istio-system # Second istio-discovery, using master version of istio TAG=latest HUB=gcr.io/istio-testing iop istio-master istio-discovery-master $IBASE/istio-control/istio-discovery \\ --set policy.enable=false \\ --set global.istioNamespace=istio-master Gateways A cluster may use multiple Gateways, each with a different load balancer IP, domains and certificates.\nSince the domain certificates are stored in the gateway namespace, it is recommended to keep each gateway in a dedicated namespace and restrict access.\nFor large-scale gateways it is optionally possible to use a dedicated pilot in the gateway namespace.\nAdditional test templates A number of helm test setups are general-purpose and should be installable in any cluster, to confirm Istio works properly and allow testing the specific install.\n"},{"uri":"https://www.didida.top/kubernetes/deployment/istio/manifests/charts/updating-charts/","title":"","tags":[],"description":"","content":"Upating charts and values.yaml The charts in the manifests directory are used in istioctl to generate an installation manifest. The configuration settings contained in values.yaml files and passed through the CLI are validated against a schema. Whenever making changes in the charts, it\u0026rsquo;s important to follow the below steps.\nStep 0. Check that any schema change really belongs in values.yaml Is this a new parameter being added? If not, go to the next step. Dynamic, runtime config that is used to configure Istio components should go into the MeshConfig API. Values.yaml is being deprecated and adding to it is discouraged. MeshConfig is the official API which follows API management practices and is dynamic (does not require component restarts). Exceptions to this rule are configuration items that affect K8s level settings (resources, mounts etc.)\nStep 1. Make changes in charts and values.yaml in manifests directory Step 2. Make corresponding values changes in ../profiles/default.yaml The values.yaml in manifests are only used for direct Helm based installations, which is being deprecated. If any values.yaml changes are being made, the same changes must be made in the manifests/profiles/default.yaml file, which must be in sync with the Helm values in manifests.\nStep 3. Update the validation schema Istioctl uses a schema to validate the values. Any changes to the schema must be added here, otherwise istioctl users will see errors. Once the schema file is updated, run:\n$ make operator-proto This will regenerate the Go structs used for schema validation.\nStep 4. Update the generated manifests Tests of istioctl use the auto-generated manifests to ensure that the istioctl binary has the correct version of the charts. These manifests can be found in gen-istio.yaml. To regenerate the manifests, run:\n$ make gen Step 5. Update golden files The new charts/values will likely produce different installation manifests. Unit tests that expect a certain command output will fail for this reason. To update the golden output files, run:\n$ make refresh-goldens This will generate git diffs in the golden output files. Check that the changes are what you expect.\nStep 6. Create a PR using outputs from Steps 1 to 5 Your PR should pass all the checks if you followed these steps.\n"},{"uri":"https://www.didida.top/kubernetes/deployment/kube/kubernetes/","title":"","tags":[],"description":"","content":"Kubernetes ArchLinux Install sudo pacman -S conntrack-tools cni-plugins socat ethtool ebtables lsmod | grep br_netfilter sudo modprobe br_netfilter echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # v1.19.8 for compatible with gitlab v13.7.4 curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/v1.19.8/bin/linux/amd64/\\{kubeadm,kubelet,kubectl\\} sudo mkdir /etc/systemd/system/kubelet.service.d sudo vi /etc/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ Wants=network-online.target After=network-online.target [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target sudo vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026#34; # This is a file that \u0026#34;kubeadm init\u0026#34; and \u0026#34;kubeadm join\u0026#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # Swap on args # Environment=\u0026#34;KUBELET_EXTRA_ARGS=--fail-swap-on=false\u0026#34; # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS # sudo vi /etc/kubernetes/kubelet.env # add --fail-swap-on=false # systemctl start containerd # systemctl start kubelet sudo systemctl start docker sudo systemctl daemon-reload sudo systemctl start kubelet #sudo kubeadm init \\ #\t--kubernetes-version=v1.19.0 \\ #\t--image-repository registry.aliyuncs.com/google_containers \\ #\t--ignore-preflight-errors=all \\ #\t--apiserver-advertise-address=192.168.0.103 \\ #\t--pod-network-cidr=10.10.0.0/16 \\ #\t--v=5 sudo kubeadm init \\ --kubernetes-version=v1.19.0 \\ --image-repository registry.aliyuncs.com/google_containers \\ --ignore-preflight-errors=Swap \\ --apiserver-advertise-address=192.168.0.103 \\ --pod-network-cidr=10.10.0.0/16 \\ --v=5 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # change network in kube-flannel.yml kubectl apply -f kube-flannel.yml # kubectl -n kube-system edit configmap kube-flannel-cfg kubectl get pods -A Commands kubectl get pods -A kubectl cluster-info kubectl describe pod coredns-6d56c8448f-sgjjz -n kube-system kubectl logs install-prometheus -n gitlab-managed-apps # allow master run pods kubectl taint nodes --all node-role.kubernetes.io/master- # for this: Warning FailedScheduling 29s (x5 over 4m58s) default-scheduler 0/1 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate. kubectl -n kube-system edit deployment kubectl -n kube-system edit deployment coredns kubectl -n kube-system edit configmap coredns forward . 114.114.114.114 # 验证域名解析方法 kubectl run busybox --restart=Never --image=busybox -- sleep 3600 kubectl exec busybox -- nslookup github.com kubectl delete pod busybox "},{"uri":"https://www.didida.top/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://www.didida.top/tags/","title":"Tags","tags":[],"description":"","content":""}]