<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on 滴滴哒的技术分享</title><link>https://www.didida.top/kubernetes/</link><description>Recent content in Kubernetes on 滴滴哒的技术分享</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© Augustu Scofield 2021</copyright><lastBuildDate>Tue, 13 Apr 2021 21:33:04 +0800</lastBuildDate><atom:link href="https://www.didida.top/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://www.didida.top/kubernetes/introduction/</link><pubDate>Tue, 13 Apr 2021 21:34:34 +0800</pubDate><guid>https://www.didida.top/kubernetes/introduction/</guid><description>Kubernetes 服务发现和负载均衡 存储编排 自动部署和回滚 自动完成装箱计算 自我修复 密钥与配置管理 Kubernetes 组件 控制平面组件（Control Plane Components）
kube-apiserver
etcd
kube-scheduler
kube-controller-manager
从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。
节点控制器（Node Controller）
负责在节点出现故障时进行通知和响应。
副本控制器（Replication Controller）
负责为系统中的每个副本控制器对象维护正确数量的 Pod。
端点控制器（Endpoints Controller）
填充端点(Endpoints)对象(即加入 Service 与 Pod)。
服务帐户和令牌控制器（Service Account &amp;amp; Token Controllers）
为新的命名空间创建默认帐户和 API 访问令牌。
cloud-controller-manager
节点控制器（Node Controller）
用于在节点终止响应后检查云提供商以确定节点是否已被删除。
路由控制器（Route Controller）
用于在底层云基础架构中设置路由。
服务控制器（Service Controller）
用于创建、更新和删除云提供商负载均衡器。
Node 组件
节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。
kubelet
kube-proxy
容器运行时（Container Runtime）
Kubernetes 支持多个容器运行环境: Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。
插件（Addons）
DNS Web 界面（仪表盘） 容器资源监控 集群层面日志 节点（node） Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/gitlab/gitlab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/gitlab/gitlab/</guid><description>Gitlab ArchLinux Install sudo pacman -S postgresql-libs yay -S gitlab Configure your /etc/webapps/gitlab/gitlab.yml Set up your redis to run on /run/redis/redis.sock or configure gitlab to use redis TCP Put a secret bytestring to /etc/webapps/gitlab/secret Copy /usr/share/webapps/gitlab/config/secrets.yml.example to /etc/webapps/gitlab/secrets.yml and configure it Setup the database: $ (cd /usr/share/webapps/gitlab &amp;amp;&amp;amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:setup) Finally run the following commands to check your installation: $ (cd /usr/share/webapps/gitlab &amp;amp;&amp;amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:env:info) $ (cd /usr/share/webapps/gitlab &amp;amp;&amp;amp; sudo -u gitlab $(cat environment | xargs) bundle exec rake gitlab:check) Optional dependencies for gitlab postgresql: database backend python-docutils: reStructuredText markup language support [installed] smtp-server: mail server in order to receive mail notifications sudo cp /usr/share/webapps/gitlab/config/secrets.</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/istio/istio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/istio/istio/</guid><description># https://istio.io/latest/docs/examples/bookinfo/ istioctl install --set profile=demo -d manifests -y kubectl label namespace default istio-injection=enabled kubectl apply -f bookinfo.yaml kubectl get services kubectl get pods kubectl exec &amp;#34;$(kubectl get pod -l app=ratings -o jsonpath=&amp;#39;{.items[0].metadata.name}&amp;#39;)&amp;#34; -c ratings -- curl -sS productpage:9080/productpage | grep -o &amp;#34;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;#34; kubectl apply -f bookinfo-gateway.yaml kubectl get gateway kubectl get svc istio-ingressgateway -n istio-system export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&amp;#39;{.spec.ports[?(@.name==&amp;#34;http2&amp;#34;)].nodePort}&amp;#39;) echo $INGRESS_PORT export INGRESS_HOST=192.168.0.10 export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT curl -s &amp;#34;http://${GATEWAY_URL}/productpage&amp;#34; | grep -o &amp;#34;&amp;lt;title&amp;gt;.</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/install-openshift/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/install-openshift/</guid><description>Installing Istio on OpenShift using Helm Note: Be aware of the platform setup required for OpenShift when installing Istio.
To install with Helm, you must first create the namespace that you wish to install in if the namespace does not exist already. The default namespace used is istio-system and can be created as follows:
kubectl create namespace istio-system The installation process using the Helm charts is as follows:
base chart creates cluster-wide CRDs, cluster bindings and cluster resources.</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme-helm3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme-helm3/</guid><description>Helm v3 support Install The Helm charts are supported both by Helm v2 and Helm v3. Please do not introduce Helm v3 specific changes as many users are still using Helm v2 and the operator is currently using the Helm v2 code to generate.
To install with Helm v3, you must first create the namespace that you wish to install in if the namespace does not exist already. The default namespace used is istio-system and can be created as follows:</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/readme/</guid><description>Istio Installer Note: If making any changes to the charts or values.yaml in this dir, first read UPDATING-CHARTS.md
Istio installer is a modular, &amp;lsquo;a-la-carte&amp;rsquo; installer for Istio. It is based on a fork of the Istio helm templates, refactored to increase modularity and isolation.
Goals:
Improve upgrade experience: users should be able to gradually roll upgrades, with proper canary deployments for Istio components. It should be possible to deploy a new version while keeping the stable version in place and gradually migrate apps to the new version.</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/updating-charts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/istio/manifests/charts/updating-charts/</guid><description>Upating charts and values.yaml The charts in the manifests directory are used in istioctl to generate an installation manifest. The configuration settings contained in values.yaml files and passed through the CLI are validated against a schema. Whenever making changes in the charts, it&amp;rsquo;s important to follow the below steps.
Step 0. Check that any schema change really belongs in values.yaml Is this a new parameter being added? If not, go to the next step.</description></item><item><title/><link>https://www.didida.top/kubernetes/deployment/kube/kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://www.didida.top/kubernetes/deployment/kube/kubernetes/</guid><description>Kubernetes ArchLinux Install sudo pacman -S conntrack-tools cni-plugins socat ethtool ebtables lsmod | grep br_netfilter sudo modprobe br_netfilter echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system # v1.19.8 for compatible with gitlab v13.7.4 curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/v1.19.8/bin/linux/amd64/\{kubeadm,kubelet,kubectl\} sudo mkdir /etc/systemd/system/kubelet.service.d sudo vi /etc/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ Wants=network-online.</description></item></channel></rss>